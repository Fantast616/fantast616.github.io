<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"blog.fantast.top","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","width":250,"display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"manual"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"utterances","storage":true,"lazyload":false,"nav":null,"activeClass":"utterances"},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN）">
<meta property="og:url" content="https://blog.fantast.top/2022/01/16/8bf646a3f959/index.html">
<meta property="og:site_name" content="Fantast&#39;s Blog">
<meta property="og:description" content="本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-01-16T04:33:19.000Z">
<meta property="article:modified_time" content="2022-08-07T14:46:29.000Z">
<meta property="article:author" content="Fantast">
<meta property="article:tag" content="Normalization">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://blog.fantast.top/2022/01/16/8bf646a3f959/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://blog.fantast.top/2022/01/16/8bf646a3f959/","path":"2022/01/16/8bf646a3f959/","title":"机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN）"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN） | Fantast's Blog</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Fantast's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Fantast's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95%E6%A6%82%E8%BF%B0"><span class="nav-text">目录概述：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-normbn"><span class="nav-text">1、Batch Norm（BN）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5"><span class="nav-text">输入：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%B4%E6%98%8E"><span class="nav-text">说明：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84bn"><span class="nav-text">卷积网络中的BN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#layer-normln"><span class="nav-text">2、Layer Norm（LN）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#mlp%E4%B8%AD%E7%9A%84ln"><span class="nav-text">MLP中的LN</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94-1"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#instance-normin"><span class="nav-text">3、Instance Norm（IN）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94-2"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#group-normgn"><span class="nav-text">4、Group Norm（GN）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B1%BB%E6%AF%94-3"><span class="nav-text">类比：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%80%BB%E4%BD%93"><span class="nav-text">总体：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conditional-instance-normcin"><span class="nav-text">5、Conditional Instance
Norm（CIN）</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Fantast"
      src="/images/avatar.webp">
  <p class="site-author-name" itemprop="name">Fantast</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">167</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">98</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/fantast416" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;fantast416" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:fantast416@gmail.com" title="E-Mail → mailto:fantast416@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.zhihu.com/people/shuo-shuo-3-41" title="https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;shuo-shuo-3-41" rel="noopener" target="_blank">ZhiHu</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="http://www.next.zju.edu.cn/" title="http:&#x2F;&#x2F;www.next.zju.edu.cn&#x2F;" rel="noopener" target="_blank">NextLab</a>
        </li>
    </ul>
  </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://blog.fantast.top/2022/01/16/8bf646a3f959/">
    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.webp">
      <meta itemprop="name" content="Fantast">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Fantast's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习基础系列笔记7——各种归一化Normalization方法（含BN、LN、IN、CIN、GN）
        </h1>

        <div class="post-meta-container">
          
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-16 12:33:19" itemprop="dateCreated datePublished" datetime="2022-01-16T12:33:19+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-08-07 22:46:29" itemprop="dateModified" datetime="2022-08-07T22:46:29+08:00">2022-08-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E2%93%B5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">⓵ 深度学习笔记</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E2%93%B5-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/Basic%E7%B3%BB%E5%88%97%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">Basic系列笔记</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

            <div class="post-description">本文介绍了常见的几种Normalization方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance Norm、Group Norm。 各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。</div>
            <div class="post-remind">Google Chrome is recommended, otherwise the image size may be wrong</div>
            <div class="post-remind">FireFox is not recommended,Because it does not support the image zoom property</div>
        </div>
        
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>本文介绍了常见的几种 <a
href="机器学习基础系列笔记2—数据表征与自动编码器.md">机器学习基础系列笔记2—数据表征与自动编码器.md</a>
Normaliza <a
href="机器学习基础系列笔记0—常见概念与函数（更新中）.md">机器学习基础系列笔记0—常见概念与函数（更新中）.md</a>
tion方式，如BatchNorm、LayerNorm、InstanceNorm、Conditional Instance
Norm、Group Norm。
各种Normalization在理论上都能够起到平滑损失函数平面的效果，加速函数的收敛效果，但是它们在机器学习的各个领域上，各有偏重与优势。</strong></p>
<h3 id="目录概述">目录概述：</h3>
<ul>
<li><strong>BatchNorm</strong>：batch方向做归一化，算N * H * W的均值,
常用于CNN等视觉识别领域，如果当Batch的尺寸比较小或是在一些动态网络中时不适用。</li>
<li><strong>LayerNorm</strong>：channel方向做归一化，算C * H *
W的均值，LN不适用于CNN等视觉识别领域，但是可在BN无法使用的领域如RNN和Batch
Size较小时进行使用。</li>
<li><strong>InstanceNorm</strong>：一个channel一个实例内做归一化，算H *
W的均值，其适用于批量较小且单独考虑每个像素点的场景中，如GAN生成网络，但在MLP或RNN或Feature
Map较小的时候不适用。</li>
<li><strong>GroupNorm</strong>：将channel方向分group，然后每个group内做归一化，算(C
 G) * H * W的均值</li>
</ul>
<h3 id="batch-normbn">1、Batch Norm（BN）</h3>
<p>​ Ioffe 和 Szegedy
的开创性工作引入了批量归一化（BN）层，通过归一化特征统计显示简化了前馈网络的训练。
BN 层最初旨在加速判别网络的训练，但也被发现在生成图像建模中有效。</p>
<h5 id="输入"><strong>输入</strong>：</h5>
<p>​ <strong>An input batch</strong> <span class="math inline">\(x \in
R^{N \times C \times H \times W}\)</span></p>
<h5 id="说明"><strong>说明</strong>：</h5>
<p>​ Batch
Normalization就是对一个Batch中的数据进行标准化，就是每一个值减去batch的均值，除以batch的标准差，计算公式如下：</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116184553370.png" /></p>
<ul>
<li><p><span class="math inline">\(\gamma , \beta \in
R^{C}\)</span>是从数据中训练得到的参数。</p></li>
<li><p><span class="math inline">\(\mu(x) , \sigma(x) \in
R^{C}\)</span>是均值和方差，为每个特征通道（C）独立计算批量大小（N）和空间维度（H
* W）</p></li>
<li><p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220116185246851.png" /></p></li>
</ul>
<h5 id="卷积网络中的bn"><strong>卷积网络中的BN</strong></h5>
<p>​
BN除了可以应用在MLP上，其在CNN网络中的表现也非常好，<strong>卷积网络和MLP的不同点是卷积网络中每个样本的隐层节点的输出是三维（宽度，高度，维度）的</strong>，而MLP是一维的。</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_31_622.png" /></p>
<p>​ 在上图中，假设一个批量有 <span
class="math inline">\(m\)</span>个样本，Feature Map的尺寸是 <span
class="math inline">\({p \times q}\)</span>，通道数是<span
class="math inline">\(d\)</span>。<strong>在卷积网络中，BN的操作是以Feature
Map为单位的</strong>，因此一个BN要统计的数据个数为 <span
class="math inline">\({m \times p \times q}\)</span>，每个Feature
Map使用一组<span class="math inline">\(\gamma\)</span>和<span
class="math inline">\({\beta}\)</span>.</p>
<h5 id="类比"><strong>类比</strong>：</h5>
<p>​ 如果把输入<span class="math inline">\(x \in R^{N \times C \times H
\times W}\)</span>类比为一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H
行，每行 W 个字符。BN
求均值时，相当于把这些书按页码一一对应地加起来（例如第1本书第36页，第2本书第36页......），再除以每个页码下的字符总数：N<strong>×</strong>H<strong>×</strong>W，因此可以把
<strong>BN
看成求“平均书”</strong>的操作（注意这个“平均书”每页只有一个字）。</p>
<h5 id="总结"><strong>总结：</strong></h5>
<p>​
BN是深度学习调参中非常好用的策略之一（另外一个可能就是Dropout），当你的模型发生<strong>梯度消失/爆炸或者损失值震荡比较严重</strong>的时候，在BN中加入网络往往能取得非常好的效果，因为BN能够起到平滑损失平面的作用。</p>
<p>​ BN也有一些不是非常适用的场景，在遇见这些场景时要谨慎的使用BN：</p>
<ul>
<li>受制于硬件限制，每个Batch的尺寸比较小，这时候谨慎使用BN；</li>
<li>在类似于RNN的<strong>动态网络</strong>中谨慎使用BN；</li>
<li>训练数据集和测试数据集方差较大的时候。</li>
</ul>
<h3 id="layer-normln">2、Layer Norm（LN）</h3>
<p>​ Layer
Normalization（LN）的提出有效的解决了BN的这两个问题（一个是不适用于动态网络，一个是batch尺寸较小的时候）。LN和BN不同点是归一化的维度是互相垂直的。如下图所示。在图1中<span
class="math inline">\(N\)</span>表示样本轴， <span
class="math inline">\(C\)</span>表示通道轴， <span
class="math inline">\(F\)</span>是每个通道的特征数量( W*H )。BN
如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_37_355.png" /></p>
<h5 id="mlp中的ln"><strong>MLP中的LN</strong></h5>
<p>​
BN的两个缺点的产生原因均是因为<strong>计算归一化统计量时计算的样本数太少</strong>。LN是一个独立于batch
size的算法，所以无论样本数多少都不会影响参与LN计算的数据量，从而解决BN的两个问题。</p>
<p>​ 先看MLP中的LN。设<span
class="math inline">\(H\)</span>是一层中隐层节点的数量， <span
class="math inline">\(l\)</span>是MLP的层数，我们可以计算LN的归一化统计量<span
class="math inline">\(\mu\)</span>和<span
class="math inline">\(\sigma\)</span>：</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_24_28_295.png" /></p>
<p>​
注意上面<strong>统计量的计算是和样本数量没有关系</strong>的，它的<strong>数量只取决于隐层节点的数量</strong>，所以只要隐层节点的数量足够多，我们就能保证LN的归一化统计量足够具有代表性。</p>
<p>​ 通过<span class="math inline">\(\mu ^{l}\)</span>和<span
class="math inline">\(\sigma
^{l}\)</span>可以计算得到归一化后的值：<span
class="math inline">\(\hat{a}^l\)</span></p>
<p>​ <img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_26_22_177.png" /></p>
<p>​
BN的文章中介绍过几乎所有的归一化方法都能起到平滑损失平面的作用。<strong>所以从原理上讲，LN能加速收敛速度的。</strong>但我们发现，将LN添加到CNN后，实验结果表明LN破坏了卷积层学习到的特征，使得模型无法收敛，所以在CNN之后使用BN是一个较好的选择。</p>
<h5 id="类比-1"><strong>类比</strong>：</h5>
<p>​ LN 求均值时，相当于把每一本书的所有字加起来，再除以这本书的
字符总数：C<strong>×</strong>H<strong>×</strong>W，即求整本书的“平均字”，求标准差时也是同理。</p>
<h5 id="总结-1"><strong>总结</strong></h5>
<p>​
总体而言，LN是和BN非常近似的一种归一化方法，不同的是<strong>BN取的是不同样本的同一个特征，而LN取的是同一个样本的不同特征</strong>。在BN和LN都能使用的场景中，<strong>BN的效果一般优于LN，原因是基于不同数据，同一特征得到的归一化特征更不容易损失信息。</strong></p>
<p>​
但是有些场景是不能使用BN的，例如batchsize较小或者在RNN中，这时候可以选择使用LN，LN得到的模型更稳定且起到正则化的作用。RNN能应用到小批量和RNN中是因为LN的归一化统计量的计算是和batchsize没有关系的</p>
<h3 id="instance-normin">3、Instance Norm（IN）</h3>
<p>​
对于图像风格迁移这类<strong>注重每个像素的任务</strong>来说，每个样本的每个像素点的信息都是非常重要的，于是像Batch
Normlization这种每个批量的所有样本都做归一化的算法就不太适用了，因为BN计算归一化统计量时考虑了一个批量中所有图片的内容，从而<strong>造成了每个样本独特细节的丢失</strong>。同理对于LayerNormalization这类需要考虑一个样本所有通道的算法来说可能忽略了不同通道的差异，也不太适用于图像风格迁移这类应用。</p>
<p>​ 所以一篇论文提出了Instance
Normalization（IN），一种更适合对单个像素有更高要求的场景的归一化算法（IST，GAN等）。IN的算法非常简单，<strong>计算归一化统计量时考虑单个样本，单个通道的所有元素</strong>。IN（右）和BN（中）以及LN（左）的不同从图1中可以非常明显的看出。<img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/屏幕捕获_2022_01_17_09_33_44_819.png" /></p>
<p>​ IN方法计算公式如下：</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093553291.png" /></p>
<p>​ 不同于BN(x),这边的<span class="math inline">\(\mu(x) , \sigma(x) \in
R^{C}\)</span>是均值和方差，是为每个实例的每个特征通道（C）独立计算的。</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093715421.png" /></p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117093735940.png" /></p>
<p>​
IN在计算归一化统计量时并没有像BN那样跨样本、单通道，也没有像LN那样单样本、跨通道。它是取的单通道，单样本上的数据进行计算。所以对比BN的公式，它只需要它只需要去掉批量维的求和即可。</p>
<h5 id="类比-2"><strong>类比</strong>：</h5>
<p>​ IN
求均值时，相当于把一页书中所有字加起来，再除以该页的总字数：H<strong>×</strong>W，即求每页书的“平均字”，求标准差时也是同理。</p>
<h5 id="总结-2"><strong>总结</strong>：</h5>
<p>​
IN本身是一个非常简单的算法，<strong>尤其适用于批量较小且单独考虑每个像素点的场景中</strong>，因为其计算归一化统计量时没有混合批量和通道之间的数据，对于这种场景下的应用，我们可以考虑使用IN。</p>
<p>​
另外需要注意的一点是在图像这类应用中，每个通道上的值是比较大的，因此也能够取得比较合适的归一化统计量。但是有两个场景建议不要使用IN:</p>
<ol type="1">
<li>MLP或者RNN中：因为在MLP或者RNN中，每个通道上只有一个数据，这时会自然不能使用IN；</li>
<li>Feature
Map比较小时：因为此时IN的采样数据非常少，得到的归一化统计量将不再具有代表性。</li>
</ol>
<h3 id="group-normgn">4、Group Norm（GN）</h3>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/n6ck05dvpj.png" /></p>
<p>​ GN 把通道分为组，并计算每一组之内的均值和方差，以进行归一化。GN
的计算与批量大小无关，其精度也在各种批量大小下保持稳定。可以看到，GN和LN很像。</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/1496926-9e0fd762d02d26c1.webp" /></p>
<h5 id="类比-3"><strong>类比</strong>：</h5>
<p>​ GN 相当于把一本 C 页的书平均分成 G 份，每份成为有 C/G
页的小册子，求每个小册子的“平均字”和字的“标准差”。</p>
<h5 id="总体"><strong>总体</strong>：</h5>
<p>​ <strong>LN 和 IN
在视觉识别上的成功率都是很有限的</strong>，对于<strong>训练序列模型（RNN/LSTM）或生成模型（GAN）</strong>很有效。所以，<strong>在视觉识别领域，BN用的比较多，GN就是为了改善BN的不足而来的。</strong></p>
<p>​
<strong>GN适用于占用显存比较大的任务，例如图像分割</strong>。对这类任务，可能
batchsize 只能是个位数，再大显存就不够用了。而当 batchsize
是个位数时，BN
的表现很差，因为没办法通过几个样本的数据量，来近似总体的均值和标准差。GN
是独立于 batch 的,所以可以适用。</p>
<h3 id="conditional-instance-normcin">5、Conditional Instance
Norm（CIN）</h3>
<p>​ Dumoulin等人在进行风格迁移任务，使用IN的时候，用不同的<span
class="math inline">\(\gamma\)</span>和 <span
class="math inline">\(\beta\)</span>即可生成出风格不同的图像，于是提出了Conditional
Instance Normalization(CIN)。</p>
<p><img
src="https://mypic416.oss-cn-hangzhou.aliyuncs.com/windows/image-20220117102111791.png" /></p>
<p>​ 其中s代表风格，<span class="math inline">\(\gamma ^s\)</span>和
<span class="math inline">\(\beta ^s\)</span>是学出来的，一组 <span
class="math inline">\((\gamma ^s,\beta
^s)\)</span>对应一种风格。Dumoulin等人的方法迁移有限种的风格，想迁移新的的风格则需要训练新的模型。</p>
<p><strong>部分内容参考链接：</strong></p>
<p>https://zhuanlan.zhihu.com/p/54530247</p>
<p>https://zhuanlan.zhihu.com/p/56542480</p>
<p>https://www.jianshu.com/p/f15fcdf13438</p>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2022\01\18\920f212c6758\" rel="bookmark">机器学习基础系列笔记9——归一化方法FRN(Filter Response Normalization)</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Normalization/" rel="tag"># Normalization</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/01/14/92b79ab7b72b/" rel="prev" title="《Deep Residual Fourier Transformation for Single Image Deblurring》论文笔记">
                  <i class="fa fa-chevron-left"></i> 《Deep Residual Fourier Transformation for Single Image Deblurring》论文笔记
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/01/17/27fde64f6abc/" rel="next" title="机器学习基础系列笔记8——图像风格迁移方法AdaIN">
                  机器学习基础系列笔记8——图像风格迁移方法AdaIN <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments utterances-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Fantast</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">761k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">35:13</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="utterances" type="application/json">{"enable":true,"repo":"Fantast416/myblog-comment","issue_term":"pathname","theme":"github-light"}</script>
<script src="/js/third-party/comments/utterances.js"></script>

</body>
</html>
